<div align="center">

# ü§ñ AI DevOps Copilot

[![CI/CD](https://img.shields.io/github/actions/workflow/status/dankbhardwaj/ai-devops-copilot/deploy.yml?branch=main&label=CI%2FCD&logo=github-actions&logoColor=white)](https://github.com/dankbhardwaj/ai-devops-copilot/actions)
[![Docker](https://img.shields.io/docker/pulls/dankbhardwaj/ai-devops-copilot?logo=docker&logoColor=white&color=2496ED)](https://hub.docker.com/r/dankbhardwaj/ai-devops-copilot)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Enabled-326CE5?logo=kubernetes&logoColor=white)](https://kubernetes.io/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?logo=python&logoColor=white)](https://www.python.org/)

**A production-grade LLMOps system integrating RAG, LLM inference, containerization, Kubernetes orchestration, and automated CI/CD pipelines.**

[üìñ Documentation](#-architecture) ‚Ä¢ [üöÄ Quick Start](#-quick-start) ‚Ä¢ [üõ† API Reference](#-api-reference) ‚Ä¢ [ü§ù Contributing](#-contributing)

---

</div>

## üìå Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [System Flow Diagrams](#-system-flow-diagrams)
- [CI/CD Pipeline](#-cicd-pipeline)
- [Kubernetes Deployment Strategy](#-kubernetes-deployment-strategy)
- [Quick Start](#-quick-start)
- [Docker Usage](#-docker-usage)
- [Kubernetes Setup](#-kubernetes-setup)
- [API Reference](#-api-reference)
- [Project Structure](#-project-structure)
- [DevOps Capabilities](#-devops-capabilities)
- [Screenshots](#-screenshots)
- [Resume Summary](#-resume-summary)
- [Author](#-author)

---

## üß† Overview

**AI DevOps Copilot** is a **Retrieval-Augmented Generation (RAG)** assistant specialized in answering DevOps, Kubernetes, and cloud infrastructure questions. It combines semantic search with large language model inference to deliver accurate, context-aware responses.

This project is a complete **end-to-end LLMOps demonstration** covering:

- üîç **Intelligent retrieval** from a vector database (ChromaDB)
- üß† **Local LLM inference** via Ollama (no cloud API costs)
- ‚ö° **High-performance REST API** using FastAPI
- üê≥ **Containerized** with Docker for portability
- ‚ò∏Ô∏è **Orchestrated** on Kubernetes with zero-downtime deployments
- üîÅ **Fully automated CI/CD** with GitHub Actions

---

## üèó Architecture

### High-Level System Architecture

```mermaid
graph TB
    subgraph CLIENT["üñ•Ô∏è Client Layer"]
        A[Web Browser / cURL / SDK]
    end

    subgraph API["‚ö° API Layer"]
        B[FastAPI Server<br/>POST /ask<br/>GET /health]
    end

    subgraph RAG["üîç RAG Pipeline"]
        C[Query Encoder<br/>Embedding Model]
        D[(ChromaDB<br/>Vector Store)]
        E[Context Retriever<br/>Top-K Results]
    end

    subgraph LLM["üß† LLM Inference"]
        F[Ollama Runtime]
        G[TinyLlama / Phi3 / Llama3]
    end

    subgraph OUTPUT["üì§ Response"]
        H[AI-Generated Answer<br/>with Context]
    end

    A -->|HTTP POST /ask| B
    B --> C
    C -->|Embed Query| D
    D -->|Semantic Search| E
    E -->|Retrieved Context| F
    F --> G
    G -->|Generated Response| H
    H -->|JSON Response| A

    style CLIENT fill:#1a1a2e,stroke:#e94560,color:#fff
    style API fill:#16213e,stroke:#0f3460,color:#fff
    style RAG fill:#0f3460,stroke:#533483,color:#fff
    style LLM fill:#533483,stroke:#e94560,color:#fff
    style OUTPUT fill:#1a1a2e,stroke:#e94560,color:#fff
```

---

## üõ† Tech Stack

| Category | Technology | Purpose |
|---|---|---|
| **API Framework** | ![FastAPI](https://img.shields.io/badge/-FastAPI-009688?logo=fastapi&logoColor=white) | REST API server & Swagger UI |
| **LLM Runtime** | ![Ollama](https://img.shields.io/badge/-Ollama-black?logo=llama&logoColor=white) | Local LLM inference engine |
| **LLM Models** | TinyLlama / Phi3 / Llama3 | Language model backends |
| **Vector DB** | ChromaDB | Semantic context retrieval |
| **Containerization** | ![Docker](https://img.shields.io/badge/-Docker-2496ED?logo=docker&logoColor=white) | Application packaging |
| **Orchestration** | ![Kubernetes](https://img.shields.io/badge/-Kubernetes-326CE5?logo=kubernetes&logoColor=white) | Container orchestration |
| **Local K8s** | kind (K8s in Docker) | Local cluster for development |
| **CI/CD** | ![GitHub Actions](https://img.shields.io/badge/-GitHub_Actions-2088FF?logo=github-actions&logoColor=white) | Automated pipelines |
| **Runner** | Self-hosted GitHub Runner | Pipeline execution agent |
| **Registry** | ![Docker Hub](https://img.shields.io/badge/-Docker_Hub-2496ED?logo=docker&logoColor=white) | Container image registry |
| **Language** | ![Python](https://img.shields.io/badge/-Python_3.11+-3776AB?logo=python&logoColor=white) | Application language |

---

## üîÑ System Flow Diagrams

### RAG Query Processing Pipeline

```mermaid
sequenceDiagram
    autonumber
    actor User
    participant API as FastAPI<br/>/ask endpoint
    participant Embed as Embedding<br/>Model
    participant VDB as ChromaDB<br/>Vector Store
    participant LLM as Ollama<br/>LLM Runtime
    participant Model as TinyLlama /<br/>Phi3 / Llama3

    User->>+API: POST /ask {"question": "How to scale pods in K8s?"}
    API->>+Embed: Encode query to vector
    Embed-->>-API: Query embedding [0.23, -0.41, ...]
    API->>+VDB: Similarity search (top-k=3)
    VDB-->>-API: Relevant context chunks
    Note over API: Build augmented prompt:<br/>Context + Question
    API->>+LLM: Send augmented prompt
    LLM->>+Model: Forward pass inference
    Model-->>-LLM: Generated tokens
    LLM-->>-API: Decoded response text
    API-->>-User: {"answer": "To scale pods, use kubectl scale..."}
```

---

### Component Interaction Map

```mermaid
graph LR
    subgraph INGESTION["üì• Data Ingestion (Offline)"]
        D1[Raw DevOps Docs]
        D2[K8s Docs]
        D3[Custom Knowledge Base]
        D4[Text Chunker & Embedder]
        D5[(ChromaDB)]
        D1 & D2 & D3 --> D4 --> D5
    end

    subgraph INFERENCE["‚ö° Inference (Online)"]
        I1[User Query]
        I2[FastAPI]
        I3[Embed Query]
        I4[Vector Search]
        I5[Prompt Builder]
        I6[Ollama]
        I7[Response]
        I1 --> I2 --> I3 --> I4 --> I5 --> I6 --> I7
    end

    D5 -->|Retrieve Context| I4

    style INGESTION fill:#1e3a5f,stroke:#4fc3f7,color:#fff
    style INFERENCE fill:#1b5e20,stroke:#81c784,color:#fff
```

---

## üîÅ CI/CD Pipeline

### GitHub Actions Workflow

```mermaid
flowchart TD
    A([üë®‚Äçüíª Developer\nPushes Code]) -->|git push origin main| B

    subgraph GHA["üîß GitHub Actions Cloud"]
        B[Trigger Workflow\ngithub.event: push]
        B --> C{Branch Check\nbranch == 'main'?}
        C -->|‚ùå No| Z([Skip Deployment])
        C -->|‚úÖ Yes| D[Checkout Repository\nactions/checkout@v3]
        D --> E[Set up Docker Buildx\ndocker/setup-buildx-action]
        E --> F[Login to Docker Hub\ndocker/login-action]
    end

    subgraph RUNNER["üñ•Ô∏è Self-Hosted Runner"]
        F --> G[Build Docker Image\ndocker build -t app:SHA]
        G --> H{Build\nSuccess?}
        H -->|‚ùå Fail| FAIL([üö® Notify Failure\nPipeline Aborted])
        H -->|‚úÖ Pass| I[Tag with Commit SHA\n& 'latest']
        I --> J[Push to Docker Hub\ndocker push]
        J --> K[Update K8s Deployment\nkubectl set image]
        K --> L[Rolling Update Triggered\nKubernetes Scheduler]
    end

    subgraph K8S["‚ò∏Ô∏è Kubernetes Cluster"]
        L --> M[Old Pod: Terminating]
        L --> N[New Pod: Pulling Image]
        N --> O{Health Check\nReadiness Probe}
        O -->|‚ùå Fail| P[Rollback Initiated\nPrevious Version Restored]
        O -->|‚úÖ Pass| Q([üü¢ Deployment Live\nZero Downtime])
    end

    style GHA fill:#1a1a2e,stroke:#4fc3f7,color:#fff
    style RUNNER fill:#0d1b2a,stroke:#e94560,color:#fff
    style K8S fill:#1b2838,stroke:#326CE5,color:#fff
```

---

## ‚ò∏Ô∏è Kubernetes Deployment Strategy

### Rolling Update Architecture

```mermaid
graph TB
    subgraph CLUSTER["‚ò∏Ô∏è Kubernetes Cluster (kind)"]

        subgraph CTRL["Control Plane"]
            API_SRV[kube-apiserver]
            SCHED[kube-scheduler]
            CM[controller-manager]
        end

        subgraph NS["Namespace: default"]
            SVC[Service\nai-devops-service\nport: 8000]

            subgraph DEPLOY["Deployment: ai-devops-copilot\nstrategy: RollingUpdate"]
                subgraph RS_NEW["ReplicaSet v2 (New)"]
                    P3[üü¢ Pod v2\nRunning]
                    P4[üü° Pod v2\nStarting]
                end
                subgraph RS_OLD["ReplicaSet v1 (Old)"]
                    P1[üî¥ Pod v1\nTerminating]
                    P2[üü¢ Pod v1\nRunning]
                end
            end
        end

        SVC --> P2
        SVC --> P3
        API_SRV --> DEPLOY
        SCHED --> P4
        CM --> RS_NEW
    end

    subgraph POLICY["üîÑ Rolling Update Policy"]
        direction LR
        RU1["maxUnavailable: 25%\n(min 1 pod always serving)"]
        RU2["maxSurge: 25%\n(max 1 extra pod during update)"]
        RU3["Zero Downtime ‚úÖ"]
        RU1 --- RU2 --- RU3
    end

    style CLUSTER fill:#0d1b35,stroke:#326CE5,color:#fff
    style CTRL fill:#0f2044,stroke:#4fc3f7,color:#fff
    style NS fill:#112244,stroke:#326CE5,color:#fff
    style DEPLOY fill:#0a1a33,stroke:#81c784,color:#fff
    style POLICY fill:#1b2838,stroke:#e94560,color:#fff
```

---

### Kubernetes Resource Topology

```mermaid
graph LR
    subgraph K8S_RES["Kubernetes Resources"]
        direction TB
        CM2[ConfigMap\napp-config]
        SEC[Secret\nregistry-credentials]
        DEP[Deployment\nai-devops-copilot\nreplicas: 2]
        SVC2[Service\nClusterIP / NodePort\n:8000]
        ING[Ingress\n/ask ‚Üí svc:8000]
        HPA[HorizontalPodAutoscaler\nmin:2 max:10\nCPU: 70%]

        CM2 --> DEP
        SEC --> DEP
        DEP --> SVC2
        SVC2 --> ING
        HPA --> DEP
    end

    style K8S_RES fill:#0d1b35,stroke:#326CE5,color:#fff
```

---

## üöÄ Quick Start

### Prerequisites

```bash
# Required tools
docker --version       # Docker 24+
kubectl version        # Kubernetes CLI
kind version           # Kind 0.20+
ollama --version       # Ollama runtime
```

### 1Ô∏è‚É£ Clone Repository

```bash
git clone https://github.com/dankbhardwaj/ai-devops-copilot.git
cd ai-devops-copilot
```

### 2Ô∏è‚É£ Start Ollama & Pull Model

```bash
# Start Ollama service
ollama serve &

# Pull a lightweight model
ollama pull tinyllama
# OR for better quality:
ollama pull phi3
```

### 3Ô∏è‚É£ Run Locally

```bash
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Open: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## üê≥ Docker Usage

### Build Image

```bash
docker build -t dankbhardwaj/ai-devops-copilot:latest .
```

### Run Container

```bash
docker run -d \
  --name ai-devops-copilot \
  -p 8000:8000 \
  --add-host=host.docker.internal:host-gateway \
  dankbhardwaj/ai-devops-copilot:latest
```

### Docker Compose (Recommended)

```bash
docker compose up -d
```

---

## ‚ò∏Ô∏è Kubernetes Setup

### Step 1 ‚Äî Create Kind Cluster

```bash
kind create cluster --name dev
kubectl cluster-info --context kind-dev
```

### Step 2 ‚Äî Load Image into Kind

```bash
kind load docker-image dankbhardwaj/ai-devops-copilot:latest --name dev
```

### Step 3 ‚Äî Deploy to Kubernetes

```bash
kubectl apply -f k8s/
```

### Step 4 ‚Äî Verify Deployment

```bash
# Check pods are running
kubectl get pods -l app=ai-devops-copilot

# Check deployment rollout
kubectl rollout status deployment/ai-devops-copilot

# Describe deployment
kubectl describe deployment ai-devops-copilot
```

### Step 5 ‚Äî Access the Application

```bash
kubectl port-forward service/ai-devops-service 8000:8000
```

Open: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## üåê API Reference

### `POST /ask`

Submit a question to the AI DevOps Copilot.

**Request:**
```json
{
  "question": "How do I perform a rolling update in Kubernetes?"
}
```

**Response:**
```json
{
  "answer": "To perform a rolling update in Kubernetes, you can update the image of your deployment using: kubectl set image deployment/<name> <container>=<new-image>. Kubernetes will then gradually replace old pods with new ones, ensuring zero downtime based on your maxUnavailable and maxSurge settings.",
  "context_used": true,
  "model": "tinyllama",
  "latency_ms": 842
}
```

### `GET /health`

Health check endpoint.

```json
{
  "status": "healthy",
  "vector_db": "connected",
  "ollama": "connected"
}
```

---

## üìÅ Project Structure

```
ai-devops-copilot/
‚îú‚îÄ‚îÄ üìÇ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application entry point
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ask.py           # /ask endpoint handler
‚îÇ   ‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py     # ChromaDB vector search
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedder.py      # Embedding model wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompt.py        # Prompt template builder
‚îÇ   ‚îî‚îÄ‚îÄ llm/
‚îÇ       ‚îî‚îÄ‚îÄ ollama_client.py # Ollama API client
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îî‚îÄ‚îÄ knowledge_base/      # DevOps & K8s documentation
‚îú‚îÄ‚îÄ üìÇ k8s/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml      # Kubernetes Deployment manifest
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml         # Kubernetes Service manifest
‚îÇ   ‚îî‚îÄ‚îÄ hpa.yaml             # HorizontalPodAutoscaler
‚îú‚îÄ‚îÄ üìÇ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ deploy.yml       # GitHub Actions CI/CD pipeline
‚îú‚îÄ‚îÄ Dockerfile               # Multi-stage Docker build
‚îú‚îÄ‚îÄ docker-compose.yml       # Local development stack
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## üéØ DevOps Capabilities Demonstrated

```mermaid
mindmap
  root((AI DevOps\nCopilot))
    LLMOps
      RAG Pipeline
      Local LLM Inference
      Vector Embeddings
      ChromaDB Integration
    Containerization
      Multi-stage Dockerfile
      Docker Compose
      Image Versioning with SHA
      Docker Hub Registry
    Kubernetes
      Kind Local Cluster
      Deployment Manifests
      Rolling Update Strategy
      Zero-Downtime Deploys
      HPA Autoscaling
    CI/CD Automation
      GitHub Actions Workflow
      Self-hosted Runner
      Automated Image Build
      kubectl Rolling Update
      Commit SHA Tagging
    API Design
      FastAPI REST API
      Swagger UI
      Health Checks
      JSON Responses
```

---

## üì∏ Screenshots

### üåê FastAPI Swagger UI
> Interactive API documentation auto-generated at `/docs`. Live `POST /ask` endpoint ready for testing with full request/response schema.

![FastAPI Swagger UI](https://raw.githubusercontent.com/dankbhardwaj/ai-devops-copilot/main/assets/fastapi-swagger-ui.png)

---

### üîÅ GitHub Actions CI/CD Pipeline
> Every push to `main` automatically triggers: Docker image build ‚Üí SHA tag ‚Üí Docker Hub push ‚Üí `kubectl set image` ‚Üí rolling update. Full automation, zero manual steps.

![GitHub Actions CI/CD](https://raw.githubusercontent.com/dankbhardwaj/ai-devops-copilot/main/assets/github-actions-ci-cd.png)

---

### ‚ò∏Ô∏è Kubernetes Pods Running
> Successful deployment confirmed. All pods in `Running` state with correct image version pulled from Docker Hub and healthy readiness probes passing.

![Kubernetes Pods Running](https://raw.githubusercontent.com/dankbhardwaj/ai-devops-copilot/main/assets/k8s-pods-running.png)

---

### üîÑ Kubernetes Rolling Update
> Live rolling update in action ‚Äî new pods spin up while old pods gracefully terminate. `maxUnavailable: 25%` and `maxSurge: 25%` ensure zero downtime throughout the entire update cycle.

![Kubernetes Rolling Update](https://raw.githubusercontent.com/dankbhardwaj/ai-devops-copilot/main/assets/k8s-rolling-update.png)

---

## üèÜ Resume Summary

> Built a **production-style LLMOps system** integrating Retrieval-Augmented Generation (RAG), FastAPI, Docker, Kubernetes, and fully automated CI/CD with rolling deployments. Demonstrated end-to-end ownership from LLM inference to zero-downtime production delivery using self-hosted GitHub Actions runners, ChromaDB vector search, and Ollama for local model inference.

**Key achievements:**
- ‚úÖ End-to-end RAG pipeline with ChromaDB and local LLM (Ollama)
- ‚úÖ Dockerized AI application with multi-stage builds and SHA-tagged versioning
- ‚úÖ Kubernetes orchestration on Kind with zero-downtime rolling updates
- ‚úÖ Fully automated CI/CD pipeline with self-hosted GitHub Actions runner
- ‚úÖ Production-ready API with health checks and Swagger documentation

---

## üìå Repository

üîó [https://github.com/dankbhardwaj/ai-devops-copilot](https://github.com/dankbhardwaj/ai-devops-copilot)

---

## ü§ù Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'feat: add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the MIT License ‚Äî see the [LICENSE](LICENSE) file for details.

---

<div align="center">

## üë®‚Äçüíª Author

**Bhaskar Sharma**

*DevOps | LLMOps | Kubernetes | CI/CD | AI Infrastructure*

[![GitHub](https://img.shields.io/badge/GitHub-dankbhardwaj-181717?logo=github)](https://github.com/dankbhardwaj)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?logo=linkedin)](https://linkedin.com/in/dankbhardwaj)

---

‚≠ê **Star this repo** if you found it useful!

*Built with ‚ù§Ô∏è for the DevOps & AI community*

</div>
