ğŸš€ AI DevOps Copilot

An end-to-end AI-powered DevOps assistant built using FastAPI, Ollama, RAG (ChromaDB), Docker, Kubernetes, and GitHub Actions CI/CD.

This project demonstrates a production-style DevOps workflow integrating AI with containerization and automated deployment.

ğŸ§  Project Overview

AI DevOps Copilot answers DevOps and Kubernetes-related questions using Retrieval-Augmented Generation (RAG).

Instead of directly querying an LLM, the system:

Retrieves relevant documentation

Passes it as context to the LLM

Generates grounded, accurate answers

This reduces hallucination and improves reliability.

ğŸ— Architecture
User Question
     â†“
FastAPI (/ask endpoint)
     â†“
Chroma Vector DB (Retrieve relevant docs)
     â†“
Ollama LLM
     â†“
Generated Answer

CI/CD Flow:

Git Push
   â†“
GitHub Actions (Self-hosted runner)
   â†“
Docker Build
   â†“
Push to Docker Hub
   â†“
kubectl set image
   â†“
Rolling Update in Kubernetes
ğŸ›  Tech Stack

Backend: FastAPI

LLM: Ollama (TinyLlama / Phi3 / Llama3)

Embeddings: nomic-embed-text

Vector DB: ChromaDB

Containerization: Docker

Orchestration: Kubernetes (kind)

CI/CD: GitHub Actions

Image Registry: Docker Hub

ğŸ“‚ Project Structure
ai-devops-copilot/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ rag/
â”‚   â””â”€â”€ agents/
â”‚
â”œâ”€â”€ docs/
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ .github/workflows/
âš™ï¸ Local Setup
1ï¸âƒ£ Clone Repo
git clone https://github.com/dankbhardwaj/ai-devops-copilot.git
cd ai-devops-copilot
2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
3ï¸âƒ£ Start Ollama
ollama run tinyllama

Ensure Ollama runs on:

http://localhost:11434
4ï¸âƒ£ Create Embeddings
python create_embeddings.py
5ï¸âƒ£ Run FastAPI
uvicorn app.main:app --reload

Open:

http://localhost:8000/docs
ğŸ³ Docker

Build image:

docker build -t dankbhardwaj/ai-devops-copilot:latest .

Run container:

docker run -p 8000:8000 \
  --add-host=host.docker.internal:host-gateway \
  dankbhardwaj/ai-devops-copilot
â˜¸ Kubernetes Deployment

Create kind cluster:

kind create cluster --name dev

Load image:

kind load docker-image dankbhardwaj/ai-devops-copilot:latest --name dev

Deploy:

kubectl apply -f k8s/

Access via port-forward:

kubectl port-forward service/ai-devops-service 8000:8000

Open:

http://localhost:8000/docs
ğŸ”„ CI/CD Pipeline

On every push to main:

Build Docker image

Tag with commit SHA

Push to Docker Hub

Update Kubernetes deployment

Trigger rolling update

Deployment strategy:

StrategyType: RollingUpdate
maxUnavailable: 25%
maxSurge: 25%
ğŸ“¦ Features

âœ” RAG-based grounded responses
âœ” Local LLM integration
âœ” Dockerized application
âœ” Kubernetes deployment
âœ” Rolling updates
âœ” Automated CI/CD
âœ” Self-hosted GitHub runner

ğŸ¯ Future Improvements

Liveness & Readiness Probes

Persistent Volume for ChromaDB

Horizontal Pod Autoscaler (HPA)

Helm packaging

Security scanning (Trivy)

Cloud deployment (EKS)

ğŸ† Resume Highlight

Built an end-to-end AI DevOps Copilot using FastAPI, Ollama (LLM), RAG (ChromaDB), Docker, Kubernetes (kind), and GitHub Actions with full CI/CD and rolling deployments.

ğŸ‘¨â€ğŸ’» Author

Bhaskar Sharma
GitHub: https://github.com/dankbhardwaj

ğŸš€ Status

Project Version: v1.0
Deployment: Automated CI/CD Enabled
Cluster: Local Kubernetes (kind)
